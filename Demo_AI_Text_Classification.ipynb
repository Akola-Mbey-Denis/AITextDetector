{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPNmhdODbvFL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8LIN4upHG_q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Subset\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjpu2_yDHBwJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM2t0-BobXV7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvdpeauOG-nM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8m4IolAbcJP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "class AITextDataset(Dataset):\n",
        "    def __init__(self, tokenizer,max_length, data_type ='train',model_type ='bert',file_path =\"./dataset/train_set.json\"):\n",
        "        super(AITextDataset, self).__init__()\n",
        "        global model\n",
        "        model  = model_type\n",
        "        self.path = file_path\n",
        "        self.data = pd.read_json(self.path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.target = self.data.iloc[:,1]\n",
        "        self.max_length = max_length\n",
        "        self.data_type = data_type\n",
        "        self.model_type = model_type\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        text = self.data.iloc[index,1]\n",
        "        if self.model_type == 'bert':\n",
        "            inputs = self.tokenizer.encode_plus(\n",
        "                text ,\n",
        "                None,\n",
        "                padding = True,\n",
        "                add_special_tokens = True,\n",
        "                return_attention_mask = True,\n",
        "                truncation = True,\n",
        "                max_length = self.max_length,\n",
        "            )\n",
        "            ids = inputs[\"input_ids\"]\n",
        "            token_type_ids = inputs[\"token_type_ids\"]\n",
        "            mask = inputs[\"attention_mask\"]\n",
        "            if self.data_type == 'train':\n",
        "                return {\n",
        "                    'text_id': self.data.iloc[index,0],\n",
        "                    'ids': torch.tensor(ids, dtype=torch.long),\n",
        "                    'mask': torch.tensor(mask, dtype=torch.long),\n",
        "                    'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "\n",
        "                    'target': torch.tensor(self.data.iloc[index, 2],dtype=torch.long)\n",
        "                    }\n",
        "            else:\n",
        "                return {\n",
        "                    'text_id': self.data.iloc[index,0],\n",
        "                    'ids': torch.tensor(ids, dtype=torch.long),\n",
        "                    'mask': torch.tensor(mask, dtype=torch.long),\n",
        "                    'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n",
        "                    }\n",
        "        else:\n",
        "            inputs = self.tokenizer.encode_plus(\n",
        "                text ,\n",
        "                None,\n",
        "                padding = True,\n",
        "                add_special_tokens = True,\n",
        "                return_attention_mask = True,\n",
        "                truncation = True,\n",
        "                max_length = self.max_length,\n",
        "            )\n",
        "         \n",
        "            ids = inputs[\"input_ids\"]\n",
        "    \n",
        "            mask = inputs[\"attention_mask\"]\n",
        "            if self.data_type == 'train':\n",
        "                return {\n",
        "                    'text_id': self.data.iloc[index,0],\n",
        "                    'ids': torch.tensor(ids, dtype=torch.long),\n",
        "                    'mask': torch.tensor(mask, dtype=torch.long),\n",
        "                    'target': torch.tensor(self.data.iloc[index, 2],dtype=torch.long)\n",
        "                    }\n",
        "            else:\n",
        "                return {\n",
        "                    'text_id': self.data.iloc[index,0],\n",
        "                    'ids': torch.tensor(ids, dtype=torch.long),\n",
        "                    'mask': torch.tensor(mask, dtype=torch.long)\n",
        "                    }\n",
        "\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn_train(data):\n",
        "        ids =[torch.tensor(d['ids']) for d in data]\n",
        "        masks =[torch.tensor(d['mask']) for d in data]\n",
        "        if model =='bert':\n",
        "            token_type_ids = [torch.tensor(d['token_type_ids']) for d in data]\n",
        "        labels = [d['target'] for d in data]\n",
        "        text_ids = [d['text_id'] for d in data]\n",
        "        ids_ = pad_sequence(ids, batch_first=True)\n",
        "        masks_ = pad_sequence(masks, batch_first=True)\n",
        "        if model =='bert':\n",
        "            token_type_ids_= pad_sequence(token_type_ids, batch_first=True)\n",
        "        labels_ = torch.tensor(labels)\n",
        "        text_id_ = torch.tensor(text_ids)\n",
        "        if model == 'bert':\n",
        "            return {\n",
        "                'text_id': text_id_,\n",
        "                'ids': ids_,\n",
        "                'mask': masks_,\n",
        "                'token_type_ids': token_type_ids_,\n",
        "                'target': labels_\n",
        "                }\n",
        "        else:\n",
        "            return {\n",
        "                'text_id': text_id_,\n",
        "                'ids': ids_,\n",
        "                'mask': masks_,\n",
        "                'target': labels_\n",
        "                }\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn_test(data):\n",
        "        ids= [torch.tensor(d['ids']) for d in data]\n",
        "        masks =[torch.tensor(d['mask']) for d in data]\n",
        "        if model =='bert':\n",
        "            token_type_ids = [torch.tensor(d['token_type_ids']) for d in data]       \n",
        "        ids_ = pad_sequence(ids, batch_first=True)\n",
        "        masks_ = pad_sequence(masks, batch_first=True)\n",
        "        if model =='bert':\n",
        "            token_type_ids_ = pad_sequence(token_type_ids, batch_first=True)\n",
        "        text_ids = [d['text_id'] for d in data]\n",
        "        text_id_ =  torch.tensor(text_ids)\n",
        "        if model == 'bert':\n",
        "            return {\n",
        "                'text_id': text_id_,\n",
        "                'ids': ids_,\n",
        "                'mask': masks_,\n",
        "                'token_type_ids': token_type_ids_\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'text_id': text_id_,\n",
        "                'ids': ids_,\n",
        "                'mask': masks_\n",
        "            }\n",
        "\n",
        "\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe-QddiRcqxx"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mouaSSToZ3YY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvg42k4Qd818"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/dataset\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH_hfc-1eJLW"
      },
      "outputs": [],
      "source": [
        "arch ='bert'\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "max_length =128\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1l31UYxeYbF"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsXQAn3kdrJr"
      },
      "outputs": [],
      "source": [
        "def validation_loop(dataloader,model,loss_fn):\n",
        "    model.eval()\n",
        "    loop = tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n",
        "    num_correct = 0\n",
        "    target_count = 0\n",
        "    total_loss  = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, dl in loop:\n",
        "            ids=dl['ids'].to(device)\n",
        "            if arch =='bert':\n",
        "                token_type_ids=dl['token_type_ids'].to(device)\n",
        "            mask= dl['mask'].to(device)\n",
        "            label=dl['target'].to(device)\n",
        "            label = label.unsqueeze(1).to(device)\n",
        "            if arch =='bert':\n",
        "                output = model(ids,mask,token_type_ids)\n",
        "            else:\n",
        "                output = model(ids,mask)\n",
        "\n",
        "            label = label.type_as(output) \n",
        "            loss = loss_fn(output,label)               \n",
        "            output =output.cpu().detach().numpy()\n",
        "            pred = np.where(output >= 0, 1, 0)\n",
        "            target_count += label.size(0)\n",
        "            \n",
        "\n",
        "            # compute accuracy per batch\n",
        "            num_correct+= sum(1 for a, b in zip(pred, label) if a[0] == b[0])\n",
        "            total_loss+= loss.item() \n",
        "       \n",
        "    print(f'Validation loss :{round(float(total_loss/len(dataloader)),3)}   with accuracy {round(float(100 * num_correct /target_count),3)}%')\n",
        "    return float(100 * num_correct / target_count)\n",
        "    \n",
        "\n",
        "def training_loop(epochs,dataloader,val_dataloader,model,loss_fn,optimizer,scheduler):\n",
        "    model.train()\n",
        "    for  epoch in range(1,epochs+1):       \n",
        "        loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n",
        "        total_loss  = 0\n",
        "        num_correct = 0\n",
        "        target_count = 0\n",
        "        for batch, dl in loop:\n",
        "            ids=dl['ids'].to(device)\n",
        "            if arch =='bert':\n",
        "                token_type_ids = dl['token_type_ids'].to(device)\n",
        "            mask= dl['mask'].to(device)\n",
        "            label=dl['target'].to(device)\n",
        "            label = label.unsqueeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            if arch =='bert':\n",
        "                output = model(ids,mask,token_type_ids)\n",
        "            else:\n",
        "                output = model(ids,mask)\n",
        "            label = label.type_as(output)\n",
        "\n",
        "            loss = loss_fn(output,label)\n",
        "            # back propagate\n",
        "            loss.backward()  \n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)          \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            output = output.cpu().detach().numpy()\n",
        "            pred = np.where(output >= 0, 1, 0)\n",
        "\n",
        "            target_count += label.size(0)\n",
        "            \n",
        "            # compute accuracy per batch\n",
        "            num_correct+= sum(1 for a, b in zip(pred, label) if a[0] == b[0]) \n",
        "              \n",
        "          \n",
        "            total_loss+= loss.item() \n",
        "      \n",
        "        print(f'Training loss :{round(float(total_loss/len(dataloader)),3)}   with accuracy {round(float(100 * num_correct /target_count),3)}%')\n",
        "        if epoch%1 == 0:\n",
        "           val_acc = validation_loop(val_dataloader,model,loss_fn)   \n",
        "           # Show progress while training\n",
        "           loop.set_description(f'Epochs={epoch}/{epochs}')\n",
        "           # saved model \n",
        "           torch.save(model.state_dict(), PATH+'/ai-text-classifier-'+str(val_acc)+'.pth')\n",
        "            \n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FMP9Tn6okgO"
      },
      "outputs": [],
      "source": [
        "class BertAITextDetector(nn.Module):\n",
        "    def __init__(self,dropout= 0.20, base_model ='bert-base'):\n",
        "        '''\n",
        "           Bert model with a new fully connected layer for text classification\n",
        "        '''\n",
        "        super(BertAITextDetector, self).__init__()\n",
        "        self.bert = baseModel(name=base_model).pretrained_model()\n",
        "        self.dropout =  dropout\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Dropout(p = self.dropout),        \n",
        "          nn.Linear(768,1)\n",
        "        )\n",
        "        self._init_weights()\n",
        "        \n",
        "    def forward(self,ids,mask,token_type_ids):\n",
        "        _,o2= self.bert(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
        "        \n",
        "        out= self.fc(o2)\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        for  m in self.fc:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        \n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "    \n",
        "\n",
        "class baseModel:\n",
        "    def __init__(self, name =\"bert-base\"):\n",
        "        self.name = name\n",
        "\n",
        "    def pretrained_model(self):\n",
        "        if self.name == 'bert-base':\n",
        "            return transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        elif self.name =='bert-large':\n",
        "            return transformers.BertModel.from_pretrained('bert-large-uncased')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvW2lSCyfHRf"
      },
      "outputs": [],
      "source": [
        "model_ = BertAITextDetector(dropout = 0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpR_6BdmfeAe"
      },
      "outputs": [],
      "source": [
        "model_.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_ce08Mzb95y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acXTNHmuf4iC"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW,get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "\n",
        "dataset = AITextDataset(tokenizer =tokenizer, max_length = 128, data_type ='train',model_type ='bert',file_path ='/content/drive/MyDrive/dataset/train_set.json')\n",
        "dataset_size = len(dataset)\n",
        "validation_split = 0.20\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "train_split = Subset(dataset, train_indices)\n",
        "\n",
        "val_split = Subset(dataset, val_indices)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=16, shuffle=True, num_workers=4,collate_fn = dataset.collate_fn_train)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_split, batch_size=16, shuffle=False, num_workers=4,collate_fn= dataset.collate_fn_train)\n",
        "##Training parameters:\n",
        "epochs = 5\n",
        "# construct an optimizer\n",
        "arch ='bert'\n",
        "optimizer = AdamW(model_.parameters(), lr=2e-5,eps = 1e-08)\n",
        "\n",
        "# Set up the learning rate scheduler\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = training_loop(epochs, train_dataloader,val_dataloader, model_, loss_fn, optimizer,scheduler)\n",
        "\n",
        "torch.save(model.state_dict(), PATH +'/ai-text-'+arch+str(epochs)+'-classifier-max-length-'+str(max_length)+'.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBFqnimBdbgM"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "dataset = AITextDataset(tokenizer = tokenizer, max_length = max_length, data_type = 'test', model_type = 'bert', file_path = '/content/drive/MyDrive/dataset/test_set.json')\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = False, collate_fn = dataset.collate_fn_test)\n",
        "model_.load_state_dict(torch.load(PATH+'/ai-text-classifier-95.5.pth'))\n",
        "model_.eval()\n",
        "model_.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with open(PATH+\"/submission--1.csv\", \"w\") as file:\n",
        "        csv_out = csv.writer(file)\n",
        "        csv_out.writerow(['id','label'])\n",
        "        loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n",
        "\n",
        "        for i,(batch, dl) in enumerate(loop):\n",
        "            ids = dl['ids'].to(device)\n",
        "            if arch =='bert':\n",
        "                token_type_ids=dl['token_type_ids'].to(device)\n",
        "            mask= dl['mask'].to(device)\n",
        "           \n",
        "            if arch =='bert':\n",
        "                output = model_(ids,mask,token_type_ids)\n",
        "            else:\n",
        "                 output=model_(ids,mask)\n",
        "          \n",
        "            output =output.logits.cpu().detach().numpy()\n",
        "            pred = np.where(output >= 0, 1, 0)  \n",
        "            row = pred.tolist()[0][0]\n",
        "            csv_out.writerow([i, row])\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sK6aGVC5Euz"
      },
      "outputs": [],
      "source": [
        "class RobertaAITextDetector(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        '''\n",
        "           Roberta model with new fully connected layer for text classification\n",
        "        '''\n",
        "        super(RobertaAITextDetector, self).__init__()    \n",
        "        self.dropout_rate = dropout    \n",
        "        self.bert = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1)\n",
        "        )\n",
        "        self._init_weights()       \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, x2 = self.bert(input_ids=input_ids, attention_mask=attention_mask,return_dict=False)\n",
        "        x = self.fc(x2)\n",
        "        return x\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        for  m in self.fc:\n",
        "            if isinstance(m, (nn.Linear,nn.LayerNorm)):\n",
        "                m.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        \n",
        "            if isinstance(m, (nn.Linear,nn.LayerNorm)) and m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWD6fMC75Eyd"
      },
      "outputs": [],
      "source": [
        "model_robert = RobertaAITextDetector(dropout = 0.20)\n",
        "model_robert.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kklHGLNP5E1s"
      },
      "outputs": [],
      "source": [
        "tokenizer =transformers.RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OJd7duG5E5Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW,get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "PATH ='/content/drive/MyDrive/dataset/robert2'\n",
        "dataset = AITextDataset(tokenizer =tokenizer, max_length = 128, data_type ='train',model_type ='robert',file_path ='/content/drive/MyDrive/dataset/train_set.json')\n",
        "dataset_size = len(dataset)\n",
        "validation_split = 0.20\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "train_split = Subset(dataset, train_indices)\n",
        "\n",
        "val_split = Subset(dataset, val_indices)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=16, shuffle=True, num_workers=4,collate_fn = dataset.collate_fn_train)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_split, batch_size=16, shuffle=False, num_workers=4,collate_fn= dataset.collate_fn_train)\n",
        "##Training parameters:\n",
        "epochs = 6\n",
        "# construct an optimizer\n",
        "arch ='robert'\n",
        "optimizer = AdamW(model_robert.parameters(), lr=2e-5,eps = 1e-08)\n",
        "\n",
        "# Set up the learning rate scheduler\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = training_loop(epochs, train_dataloader,val_dataloader, model_robert, loss_fn, optimizer,scheduler)\n",
        "\n",
        "torch.save(model.state_dict(), PATH +'/ai-text-'+arch+str(epochs)+'-classifier-max-length-'+str(max_length)+'.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZDY2kobACWO"
      },
      "source": [
        "**Robert model inference -test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jslGiX0B5FBe"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "dataset = AITextDataset(tokenizer = tokenizer, max_length = max_length, data_type = 'test', model_type = 'robe', file_path = '/content/drive/MyDrive/dataset/test_set.json')\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = False, collate_fn = dataset.collate_fn_test)\n",
        "# model_robert.load_state_dict(torch.load(PATH+'/ai-text-classifier-95.5.pth'))\n",
        "model_robert.eval()\n",
        "model_robert.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with open(PATH+\"/submission-robert2.csv\", \"w\") as file:\n",
        "        csv_out = csv.writer(file)\n",
        "        csv_out.writerow(['id','label'])\n",
        "        loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n",
        "\n",
        "        for i,(batch, dl) in enumerate(loop):\n",
        "            ids = dl['ids'].to(device)\n",
        "            if arch =='bert':\n",
        "                token_type_ids=dl['token_type_ids'].to(device)\n",
        "            mask= dl['mask'].to(device)\n",
        "           \n",
        "            if arch =='bert':\n",
        "                output = model_robert(ids,mask,token_type_ids)\n",
        "            else:\n",
        "                 output=model_robert(ids,mask)\n",
        "          \n",
        "            output =output.cpu().detach().numpy()\n",
        "            pred = np.where(output >= 0, 1, 0)  \n",
        "            row = pred.tolist()[0][0]\n",
        "            csv_out.writerow([i, row])\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUcx7uH5_6-J"
      },
      "source": [
        "**GPT model for Text Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPj30Qope7zC"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Model\n",
        "class GPTAITextDetector(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        '''\n",
        "           GPT2 model with new fully connected layer for text classification\n",
        "        '''\n",
        "        super(GPTAITextDetector, self).__init__()    \n",
        "        self.dropout_rate = dropout \n",
        "        self.base =  GPT2Model.from_pretrained('gpt2')\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1)\n",
        "        )\n",
        "        \n",
        "        self._init_weights()       \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        '''\n",
        "        Inspire by :\n",
        "                    https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L1328\n",
        "        \n",
        "        '''\n",
        "        x = self.base(input_ids=input_ids, attention_mask=attention_mask,return_dict=True)\n",
        "        sequence = x[0]\n",
        "   \n",
        "        logits = self.fc(sequence)\n",
        "        if input_ids is not None:\n",
        "            batch_size, sequence_length = input_ids.shape[:2]\n",
        "\n",
        "        if self.base.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids is not None:\n",
        "                sequence_lengths = torch.ne(input_ids, self.base.config.pad_token_id).sum(-1) - 1\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "        \n",
        "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
        "        return pooled_logits\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        for  m in self.fc:\n",
        "            if isinstance(m, (nn.Linear,nn.LayerNorm)):\n",
        "                m.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        \n",
        "            if isinstance(m, (nn.Linear,nn.LayerNorm)) and m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZNDj-1jfF3l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dAw312cfGmr"
      },
      "outputs": [],
      "source": [
        "model_gpt = GPTAITextDetector(dropout = 0.20)\n",
        "model_gpt.cuda()\n",
        "\n",
        "PATH = '/content/drive/MyDrive/dataset/gpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4MqOedJ_RqY"
      },
      "outputs": [],
      "source": [
        "# Get model's tokenizer.\n",
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model_gpt.base.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdh9OGZDfG1b"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW,get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "\n",
        "dataset = AITextDataset(tokenizer =tokenizer, max_length = 128, data_type ='train',model_type ='gpt',file_path ='/content/drive/MyDrive/dataset/train_set.json')\n",
        "dataset_size = len(dataset)\n",
        "validation_split = 0.20\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "train_split = Subset(dataset, train_indices)\n",
        "\n",
        "val_split = Subset(dataset, val_indices)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_split, batch_size=16, shuffle=True, num_workers=4,collate_fn = dataset.collate_fn_train)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_split, batch_size=16, shuffle=False, num_workers=4,collate_fn= dataset.collate_fn_train)\n",
        "##Training parameters:\n",
        "epochs = 5\n",
        "# construct an optimizer\n",
        "arch ='gpt'\n",
        "optimizer = AdamW(model_gpt.parameters(), lr=2e-4,eps = 1e-08)\n",
        "\n",
        "# Set up the learning rate scheduler\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = training_loop(epochs, train_dataloader,val_dataloader, model_gpt, loss_fn, optimizer,scheduler)\n",
        "\n",
        "torch.save(model.state_dict(), PATH +'/ai-text-'+arch+str(epochs)+'-classifier-max-length-'+str(max_length)+'.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDvuIuSufG6q"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "dataset = AITextDataset(tokenizer = tokenizer, max_length = max_length, data_type = 'test', model_type = 'gpt', file_path = '/content/drive/MyDrive/dataset/test_set.json')\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = False, collate_fn = dataset.collate_fn_test)\n",
        "\n",
        "model_gpt.eval()\n",
        "model_gpt.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with open(PATH+\"/submission-gpt.csv\", \"w\") as file:\n",
        "        csv_out = csv.writer(file)\n",
        "        csv_out.writerow(['id','label'])\n",
        "        loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n",
        "\n",
        "        for i,(batch, dl) in enumerate(loop):\n",
        "            ids = dl['ids'].to(device)\n",
        "            if arch =='bert':\n",
        "                token_type_ids=dl['token_type_ids'].to(device)\n",
        "            mask= dl['mask'].to(device)\n",
        "           \n",
        "            if arch =='bert':\n",
        "                output = model_robert(ids,mask,token_type_ids)\n",
        "            else:\n",
        "                 output=model_robert(ids,mask)\n",
        "          \n",
        "            output =output.cpu().detach().numpy()\n",
        "            pred = np.where(output >= 0, 1, 0)  \n",
        "            row = pred.tolist()[0][0]\n",
        "            csv_out.writerow([i, row])\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcE2gSUWfHG6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Julia 1.8.4",
      "language": "julia",
      "name": "julia-1.8"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
